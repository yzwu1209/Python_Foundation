{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Fundamentals for Data Science: Final Exam\n",
    "\n",
    "\n",
    "## Instructions\n",
    "The final exam is designed to evaluate your grasp of Python theory as well as Python coding.\n",
    "\n",
    "- This is an individual exam.\n",
    "- You have 24 hours to complete the exam, starting from the point at which you first access it.\n",
    "- You will be graded on the quality of your answers.  Use clear, persuasive arguments based on concepts we covered in class.\n",
    "- Please double-click the markdown cells where it says \"Your answer here\" to input answers (if you need more cells please make them markdown cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOUR NAME HERE: Youzhi Chloe Wu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: General Questions (21 pts )\n",
    "\n",
    "a) The following method is part of a larger program used by a mobile phone company.  It will work when an object of type MobileDevice or of type ServiceContract is passed in.  This is a demonstration of (select all that apply and state a reason why it applies):\n",
    "\n",
    "    1. Inheritance\n",
    "    2. Polymorphism\n",
    "    3. Duck typing\n",
    "    4. Top-down design\n",
    "    5. Functional programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Method:\n",
    "\n",
    "def add_to_cart(item):\n",
    "    cart.append(item)\n",
    "    total += item.price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a) Your answer here\n",
    "\n",
    "> This is a demonstration of 3. Duck typing. \n",
    "   - Duck typing: Python does not check variable types before running a function. The add_to_card() method works for both MobileDevice and ServiceContract objects. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Suppose you have a long list of digits (0-9) that you want to write to a file.  From a storage standpoint, would it be more efficient to use ASCII or UTF-8 as an encoding? How could you create an even smaller file to store the information?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- b) Your answer here\n",
    "\n",
    "> It would be more efficient to use ASCII from storage standpoint, because ASCII uses 7 bits to represent a character, whereas a character may occupy a minimum of 8 bits in UTF-8. We could apply Hoffman coding algorithm to compress the file to an even smaller size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) You are part of a team working on a spreadsheet program that is written in Python 3.  The program includes several classes to represent different types of objects that fit into a cell of a spreadsheet.  Give a strong argument for why your team should write an abstract base class to represent such objects and give examples of what should go into such an abstract base class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- c) Your answer here\n",
    "\n",
    "> One of the reasons is that the abstract base class which will contain all of the common shared properties, methods, etc. All sub classes can all automatically inherit the properties/methods of the base class. Another benefit would be if you need to make a change you only need to make it in one place, instead of in all sub-classes. These different types of objects all share same properties to fit into a cell of a spreadsheet. Constructing a base class to allow these different objects to inherit commonly shared properties/methods is necessary. \n",
    "\n",
    "> The base class should include: an initializer of the base class, attributes / properties / methods that will be shared among sub-classes, such as fitting into a cell of a spreadsheet.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Explain why NumPy is better than lists for \"vectorized\" math operations. Give an example of an operation that is either impossible or painful to implement using traditional Python lists compared to NumPy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- d) Your answer here\n",
    "\n",
    "    -  NumPy has n-dimensional array object which is a container for large data sets allowing for fast reading/writing. Because NumPy arrays are homogeneous, the operations taken on the array could be tremendously speed up. Whereas in the case of list, the data type of every one of the items needs to be checked first. \n",
    "    -  Also, NumPy arrays enable you to perform mathematical operations on whole blocks of data instead of looping through each element of a list. \n",
    "    -  Many mathematical operations taken on NumPy arrays match exactly the behavior seen in linear algebra. Whereas if using list, special functions will need to be defined to loop through each element to achieve the same results. \n",
    "    -  Example: To change the shape of a n-by-n matrix, you can just use NumPy reshape() method; whereas, it would be quite painful to achieve this by using Python list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) We want a list of the numbers that are the square of nonnegative integer less than 10, but whose squares are greater than 10.  Fill in a list comprehension below so that we get the desired output: [16, 25, 36, 49, 64, 81]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 25, 36, 49, 64, 81]\n"
     ]
    }
   ],
   "source": [
    "# 1e Your code here\n",
    "print([x**2 for x in range(10) if x**2 > 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Explain why the following code prints what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'function'>\n"
     ]
    }
   ],
   "source": [
    "def f(): pass\n",
    "print(type(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- f) Your answer here\n",
    "\n",
    "> f is defined as a function, although it does not do anything. That's why when printing the type of f (the variable name for this defined function), it is printed as a 'function'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) Explain why the following code prints something different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "def f(): pass\n",
    "print(type(f()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- g) Your answer here\n",
    "\n",
    "> The difference between g) and f) is that in g), it is printing the type of the output of f function. As defined in f, the function does not return anything. That's why the type of its output is NoneType. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Data Integrity (25 pts)\n",
    "\n",
    "a) Why is it important to sanity-check your data before you begin your analysis? What could happen if you don't?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- a) Your answer here\n",
    "\n",
    "> There are many reasons that sanity check before analysis is very important:\n",
    "- There may be mismatches in data type, or variations in how values are entered, or missing values in the variables that you would like to analyze. These scenarios could easily mess up your analysis. \n",
    "- There could be duplicate or outliers that may distort your analysis. Although one does need to take caution in evaluating and manipulating duplicates/outliers. \n",
    "\n",
    "> If you do not conduct sanity check, you may run into error when conducting analysis, or your analysis may be distorted by ourliers/duplicates/null values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Explain, in your own words, why real-world data is often messy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- b) Your answer here\n",
    "\n",
    "> Real-world data is most likely entered by human or collected based off human's activities. Human's interpretations of the same data field could be different. Even if it happens that human's understanding is consistent, their data entry behavior would still be very different unless a rigid data validation control is in place at data collection. Besides, there are many scenarios where data could not be collected or collected in the wrong format due to unexpected situations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) How do you determine which variables in your dataset you should check for issues prior to starting an analysis? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- c) Your answer here\n",
    "\n",
    "> First decide on which variables you would use in your analysis. These are variables not only directly presented in your analysis report, but also those related to the presented variables. Once you have that, you should check for issues for all the variables that will be involved in your analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) How do you know when you have adequately checked these variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- d) Your answer here\n",
    "\n",
    "> I think theoretically it is never adequate. But after initial data cleansing, such as removing missing values / mismatched / duplicates / outliers that do not make common sense, you can begin data analysis. The data checking process is iterative and progressive. After some data analysis, such as distribution plotting, you need to ask if the results are following common sense, if not, why not. By asking these questions throughout your analysis, it could further trigger you to conduct some more in-depth checking. Or, you would need to make reasonable assumptions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Is it possible to fully vet your data for errors before you begin your analysis? If not, what should you be looking out for while you complete your analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- e) Your answer here\n",
    "\n",
    "> I think it most likely is not possible to fully vet data for errors before analysis. Therefore, vetting data for errors should always be an integral part of analysis. You should look for odd data distribution, peculiar outliers, graphs/results that do not make common sense throughout your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3:  Elections (24 pts)\n",
    "\n",
    "Consider the following data frame in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>delegates</th>\n",
       "      <th>color</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>marco</td>\n",
       "      <td>165</td>\n",
       "      <td>blue</td>\n",
       "      <td>FL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jeb</td>\n",
       "      <td>0</td>\n",
       "      <td>red</td>\n",
       "      <td>FL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chris</td>\n",
       "      <td>0</td>\n",
       "      <td>white</td>\n",
       "      <td>NJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>donald</td>\n",
       "      <td>1543</td>\n",
       "      <td>white</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ted</td>\n",
       "      <td>559</td>\n",
       "      <td>blue</td>\n",
       "      <td>TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>john</td>\n",
       "      <td>161</td>\n",
       "      <td>red</td>\n",
       "      <td>OH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name  delegates  color state\n",
       "0   marco        165   blue    FL\n",
       "1     jeb          0    red    FL\n",
       "2   chris          0  white    NJ\n",
       "3  donald       1543  white    NY\n",
       "4     ted        559   blue    TX\n",
       "5    john        161    red    OH"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "# creating a data frame from scratch - list of lists\n",
    "\n",
    "data = [ ['marco', 165, 'blue', 'FL'], \n",
    "         ['jeb', 0, 'red', 'FL'], \n",
    "         ['chris', 0, 'white', 'NJ'], \n",
    "         ['donald', 1543, 'white', 'NY'],\n",
    "         ['ted', 559, 'blue', 'TX'],\n",
    "         ['john', 161, 'red', 'OH']\n",
    "       ]\n",
    "\n",
    "# create a data frame with column names - list of lists\n",
    "\n",
    "col_names = ['name', 'delegates', 'color', 'state']\n",
    "df = pandas.DataFrame(data, columns=col_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Using bracket indexing in Pandas, show how many delegates `ted` got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    559\n",
       "Name: delegates, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3a) Your code here\n",
    "df[df.name == 'ted']['delegates']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Using filtering in Pandas, show how many total delegates were obtained by candidates whose favorite color is blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "724"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3b) Your code here\n",
    "df[df.color == 'blue']['delegates'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Using groupby and aggregate in Pandas, show how many total delegates were obtained by candidates grouped by favorite color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>delegates</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>blue</th>\n",
       "      <td>724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>red</th>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>white</th>\n",
       "      <td>1543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       delegates\n",
       "color           \n",
       "blue         724\n",
       "red          161\n",
       "white       1543"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3c) Your code here\n",
    "df.groupby('color').agg({'delegates':'sum'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Clinical disease data (30 pts)\n",
    "\n",
    "Your boss comes to you Monday morning and says “I figured out our next step; we are going to pivot from an online craft store and become a data center for genetic disease information! I found **ClinVar** which is a repository that contains expert curated data, and it is free for the taking. This is a gold mine! Take a week and tell me what gene and mutation combinations are classified as dangerous.”\n",
    "\n",
    "1)  Look at the data set and develop a plan of action to use python to extract and summarize just what your boss wants. **Don’t code**. You can use pseudocode and/or and essay format to generate a plan in 500 words or fewer. \n",
    "\n",
    "2) Tell us the output that you expect from your planned code\n",
    "\n",
    "**Hints:**  \n",
    "\n",
    "* Look at the below sample file carefully. What fields do you want to extract? Are they in the same place every time? What strategy will you use to robustly extract and filter your data of interest? How do you plan to handle missing data?\n",
    "\n",
    "* Filter out junk. Just focus on what your boss asked for (1) gene name (2) mutation reference. (3) Filter your data to include only mutations that are dangerous as you define it. \n",
    "\n",
    "* Pandas and NumPy parsers correctly recognize the end of each line in in the ClinVar file.\n",
    "\n",
    "* The unit of observation of this dataset is one row per mutation.\n",
    "\n",
    "* While you shouldn't code your analysis, creating a few lines of code while you think through the problem may be helpful (so that you can sanity check that your plan works). So you can experiment, we have included the data file below as a Tab Separated Value file \"Genomics_Questions.txt\". Please do not submit any such code. For example, if I wanted to check that I accurately understand the \"split\" function in the context of this data, I could type:\n",
    "\n",
    "```python\n",
    "sample = \"abc;def;asd\"\n",
    "test = sample.split(';')\n",
    "```\n",
    "* This is similar to a task that one of us tackled at work. You can answer the question with the information provided below or you can look up things like `VCF file specification`. Our goal is to see that you can put together a sensible plan, describe the parsing strategy, and document and justify the decisions that you made.\n",
    "\n",
    "**This is a planning question we want you to lay out a plan in text not code.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### VCF file description (Summarized from version 4.1)\n",
    "\n",
    "```\n",
    "* The VCF specification:\n",
    "\n",
    "VCF is a text file format which contains meta-information lines, a header line, and then data lines each containing information about a position in the genome. The format also can contain genotype information on samples for each position.\n",
    "\n",
    "* Fixed fields:\n",
    "\n",
    "There are 4 fixed fields per record. All data lines are **tab-delimited**. In all cases, missing values are specified with a dot (‘.’). \n",
    "\n",
    "1. CHROM - chromosome number\n",
    "2. POS - position DNA nuceleotide count (bases) along the chromosome\n",
    "3. ID - The unique identifier for each mutation\n",
    "4. INFO - a semicolon-separated series of  keys with values in the format: <key>=<data>, and specified as <key>=<data name>[data value definition].\n",
    "\n",
    "```\n",
    "### INFO field specifications\n",
    "\n",
    "```\n",
    "GENEINFO = <Gene symbol>\n",
    "CLNSIG =  <Variant Clinical Significance (Severity)>\n",
    "  0 – unknown\t(Uncertain significance)\n",
    "  1 – untested\t(not provided)\n",
    "  2 - non-pathogenic\t(Benign)\n",
    "  3 - probable-non-pathogenic\t(Likely benign)\n",
    "  4 - probable-pathogenic\t(Likely pathogenic)\n",
    "  5 – pathogenic\t(Pathogenic)\n",
    "  6 - drug-response\t(drug response)\n",
    "  7 – histocompatibility\t(histocompatibility)\n",
    "  255 - other\t(other)\n",
    "CLNDBN = <Disease name>\n",
    "\n",
    "```\n",
    "\n",
    "### Representative/Sample ClinVar data (vcf file format)\n",
    "\n",
    "```\n",
    "##fileformat=VCFv4.0\t\t\t\t\t\t\t\n",
    "##fileDate=20160705\t\t\t\t\t\t\t\n",
    "##source=ClinVar and dbSNP\t\t\t\t\t\t\t\n",
    "##dbSNP_BUILD_ID=147\t\t\t\t\t\t\t\n",
    "#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\n",
    "1\t949523\trs786201005\tC\tT\t.\t.\tGENEINFO=ISG15;CLNSIG=5\n",
    "1\t949696\trs672601345\tC\tCG\t.\t.\tGENEINFO=ISG15;CLNSIG=5;CLNDBN=Cancer\n",
    "1\t949739\trs672601312\tG\tT\t.\t.\tGENEINFO=ISG15;CLNDBN=Cancer\n",
    "1\t955597\trs115173026\tG\tT\t.\t.\tGENEINFO=AGRN;CLNSIG=2; CLNDBN=Cancer\n",
    "1\t955619\trs201073369\tG\tC\t.\t.\tGENEINFO=AGG;CLNDBN=Heart_dis \n",
    "1\t957640\trs6657048\tC\tT\t.\t.\tGENEINFO=AGG;CLNSIG=3;CLNDBN=Heart_dis \n",
    "1\t976059\trs544749044\tC\tT\t.\t.\tGENEINFO=AGG;CLNSIG=0;CLNDBN=Heart_dis \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Plan of Action for ClinVar \n",
    "\n",
    "## 1. Data Exploration\n",
    "> ClinVar data is presented in VCF file format. The question we would like to answer by this plan is \"what gene and mutation combinations are classified as dangerous\". Therefore, our data extraction and analysis will mainly focus on data field that could answer this question. By examing the sample file, we would like to extract the following fields:\n",
    "- __CHROM__: The chromosome number\n",
    "- __ID__: The unique identifier for each mutation\n",
    "- __POS__: position DNA nuceleotide count (bases) along the chromosome. POS data field is needed in that for certain mutations, the gene has the same symbol (or name), but their mutations are different and the gene's position is different. To accurately identify the genetic information of a mutation, POS should be included. With CHROM and POS, we could accurately identify the mutation position. \n",
    "- __INFO__: which includes (1) _GENEINFO_: Gene symbol; (2) _CLNSIG_: Clinical Significance (Severity); (3) _CLNDBN_: Disease name\n",
    "\n",
    "> Next we would like to define \"dangerous mutation\" and filter to present only our data of interest. The VCF file specifications provides 0 to 7 as well as 255 (Other) as the identifications of clinical severity. However, we notice that some mutations are missing CLNSIG values; whereas CLNDBN information is provided. Therefore, we would combine two variables CLNSIG and CLNDBN to evaluate the danger level of a mutation. In our definition, a mutation is considered as \"dangerous\" if it meets any of the following criteria:\n",
    "1. CLNSIG value is 4 or 5;\n",
    "2. It has CLNDBN value; and its CLNSIG value is either missing, or not 2, 3, 6, 7. \n",
    "\n",
    "> Based off this direction, in our next section, we will provide details on how we will prepare, filter and format our data in a way for our data analysis. \n",
    "\n",
    "\n",
    "## 2. Data Preparation\n",
    "> We will take the following steps to prepare our data for analysis: \n",
    "> 1. Install PyVCF and import vcf.  \n",
    "> 2. Use vcf.Reader to read in VCF file and get an iterable of Record instances. The module allows us to access the attributes of Record object easily. \n",
    "> 3. Semicolon-delimited lists of key=value pairs are converted to Python dictionaries. Therefore, we are able to access values within INFO data field. \n",
    "> 4. Import vcf.filters, use existing filters or create customized filters to only return records that are considered as \"dangerous\" as defined in Section 2. \n",
    "> 5. Convert the filtered records into pandas dataframe for our data analysis. \n",
    "> 6. Examine our final dataframe. It is possible that the key data fields that we would like to present (as listed in Section 2) are missing. Below we summarize modifications of the data in case of missing values: \n",
    "    - Missing ID: in this scenario, we may consider create a separate identification numbering to differentiate them from those with ID values, i.e. their ID may follow \"Unknown-001\" convention. \n",
    "    - Missing CLNSIG: we will assign \"Unknown\" value to these records.\n",
    "    - Missing CLNDBN: we will assign \"Unknown\" value to these records.\n",
    "\n",
    "\n",
    "## 3. Data Visualization\n",
    "> In our data visualization, we would list our dangerous mutations in a tabular format with the key data fields presented. Along with the table, we will plot two distribution graphs to show dangerous mutations by their CLNSIG, and by CLNDBN.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
