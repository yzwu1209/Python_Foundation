{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>test of creating a paragraph byte map for Information Retrieval</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a set of n properties of a document, for example, a list of subjects contained in an entire document collection, and a number of documents, create a n x m matrix.  Each vector represents some subject or facet of interest.  For instance, [cats, dogs, birds, red_things, and so on] and each of these terms have their own matrix (cat is in 0, dogs in 1) and so on.   An algorithm or human judge reads the document [a document could be some part, such as an abstract, title, individual paragraphs, entire sections, entire document (or the equivalent depending on the medium)] and a binary value (0 = this facet is not in this unit; 1 = this facet is in this unit).  The result looks something like:\n",
    "0, 1, 0, 0, 0, 0, 1, 1, 1, 0, .... \n",
    "<br />\n",
    "Once the matrix is created ... us it to find intersections; but also let's use it to winnow (reduce the space of data) to be inspected to be included.  Pretend we have millions of records coming in rapidly to be processed.  We need some technique to pre-process the data as fast as we can to reduce the set that is actually used for other analysis.  What's more, the ever-streaming data set must be available simultaneously for other reductions and analysis.\n",
    "<br />\n",
    "Write a script and time several runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_add(*args): return bin(sum(int(x, 2) for x in args))[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'100101'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1 = '010010101101010100110101010101010100111111111'\n",
    "doc2 = '000000000000000000001000000000000000000010001'\n",
    "doc3 = '110101010101111111000000000000000000000000000'\n",
    "\n",
    "bin_add(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10284947909119\n",
      "17592202821649\n",
      "29325902479360\n",
      "11733716434944\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(int('010010101101010100110101010101010100111111111',2))\n",
    "print(int('100000000000000000001000000000000000000010001',2))\n",
    "print(int('110101010101111111000000000000000000000000000',2))\n",
    "print(int('010101010101111111000000000000000000000000000',2))\n",
    "print(int('00000000',2))\n",
    "print(int('00000001',2))\n",
    "print(int('00000010',2))\n",
    "print(int('00000011',2))\n",
    "print(int('00000100',2))\n",
    "print(int('00000101',2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0                                                  1\n",
      "0  title  [A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, ...\n",
      "1     t1  [0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, ...\n",
      "2     t2  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "3     t3  [1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, ...\n"
     ]
    }
   ],
   "source": [
    "# Do the same as a pandas dataframe\n",
    "# for varieties of approaches see http://pbpython.com/pandas-list-dict.html\n",
    "import pandas as pd\n",
    "\n",
    "#\n",
    "items = [('title',['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']),\n",
    "         ('t1',[0,1,0,0,1,0,1,0,1,1,0,1,0,1,0,1,0,0,1,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,0,1,1,1,1,1,1,1,1,1]),\n",
    "         ('t2',[1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1]),\n",
    "         ('t3',[1,1,0,1,0,1,0,1,0,1,0,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]),\n",
    "        ]\n",
    "df = pd.DataFrame.from_dict(items)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />We can import using lists and dictionaries ... but also other technqiues, to be sure.  from_items is probably more common.  Now the challenge is to extract a subset of data that conform to our needs.  For instance, say columns A and G are conceptually related for this project and we want to extract them.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>A challenge ... </h2>\n",
    "Imagine we have x number of titles (or documents, represented above by t<i>n</i>) and a series of facets (or subjects or whatever interests your group has; A - ...).  \n",
    "Now, say you need to extract all the matrices in A and G (because you have back-end data that says A and G are related to some need).  Once you've extracted A and G, can you determine which of the 2 has a total higher value?  [That means we start of binarily ... 1/0 if the topic A or G are there ...] and then we have to find facets that are conceptually close to A|G (here you can make up values, such as B and Q).  Then bit-wise sum of the original A|G and subsets and then \"rank by relevancy.\"  \n",
    "<br />\n",
    "The concept of \"relevancy\" is the very heart of search engine differences. In addition, there is also a \"weight scheme\" that's included.  For instance, if A is conceptually related to B, C, F, Q, and X then how do we integrate them all and then normalize 'em...  and given the variance in these data, what kinds of weight schemes would we want to add (that is, are there metadata or other inputs to include?  [E.g., PageRank's weighting scheme]).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>How fast and how else?</h4>\n",
    "Here are 2 other challenges:  (a) can you create a group of data, import them as a list and \n",
    "    as a dictionary, and then time the differences in retrieval speed?\n",
    "    Another real world question is how to reduce the volume of data before processing?  For instance, what's the probability of a set of n facets being sufficiently close to include in further processing?  [This is a real world question posed to me yesterday by a Boston company.]\n",
    "\n",
    "<h3>LMK!</h3>\n",
    "This is <i>entirely</i> optional - a fun thing to play with ... cheers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea from this comes from (the hazy remembrance from grad school) Frakes &amp; Baeza-Yates&rsquo; <i>Information retrieval: data structures &amp; algorithms</i>.  And someone has posted this text online!  [https://theswissbay.ch/pdf/Gentoomen%20Library/Information%20Retrieval/Information%20Retrieval%20Data%20Structures%20And%20Algorithms_FRAKES%20WB%20%282004%29.pdf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
